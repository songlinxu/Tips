The rapid advancement of large language models has been diversely applied into in HCI, across a wide range of applications in social behaviors (Generative Agents \cite{park2023generative}), virtual reality \cite{wan2024building} with tour guidance (VirtuWander \cite{wang2024virtuwander}), Human-AI collaboration (AI Chains \cite{wu2022ai}, \cite{wang2024human}), creative tasks (CharacterMeet \cite{qin2024charactermeet}, Luminate \cite{suh2024luminate}, C2Ideas \cite{hou2024c2ideas}, ABScribe \cite{reza2024abscribe}, AngleKindling \cite{petridis2023anglekindling}, PopBlends \cite{wang2023popblends}), healthcare (MindfulDiary \cite{kim2024mindfuldiary}, ChaCha \cite{seo2024chacha}, Narrating Fitness \cite{stromel2024narrating}, \cite{rajashekar2024human}) with health intervention (MindShift \cite{wu2024mindshift}, \cite{jo2024understanding, jo2023understanding, calle2024towards, ma2024evaluating}), web interaction \cite{deng2024large} with UI design (ReactGenie \cite{yang2024reactgenie}, \cite{duan2024generating}, \cite{wang2023enabling}), coding (CollabCoder \cite{gao2024collabcoder}, \cite{liu2023wants}), behavioral change (CatAlyst \cite{arakawa2023catalyst},\cite{bhattacharjee2024understanding}) with human augmentation (Memoro \cite{zulfikar2024memoro}, \cite{jang2024s}), business (Marco \cite{fok2024marco}), and so on.

% Agents for Education

In educational context, LLMs-powered agents have been utilized to serve as both teachable agents (Mathemyths \cite{zhang2024mathemyths}, \cite{jin2024teach}, \cite{liang2023let}) to provide instructions \cite{vadaparty2024cs1}, recommend learning concepts \cite{li2024learning}, and give feedback \cite{matelsky2023large}. Specifically, DevCoach \cite{wang2024devcoach} supports students' learning in software development at scale with LLMs-powered generative agents and ReadingQizMaker \cite{lu2023readingquizmaker} proposes a Human-NLP collaborative system to support instructors to design high-quality reading quiz. In programming education, CodeTailor \cite{hou2024codetailor} uses LLMs-powered personalized parsons puzzles to support engagement during students' learning in programming and PaTAT\cite{gebreegziabher2023patat} presents a Human-AI collaborative qualitative coding system with explainable interactive rule synthesis with LLMs.

@inproceedings{belkin2002using,
  title={Using manifold stucture for partially labeled classification},
  author={Belkin, Mikhail and Niyogi, Partha},
  booktitle={Advances in neural information processing systems},
  pages={929--936},
  year={2002}
}

@article{xu2024eduagent,
  title={EduAgent: Generative Student Agents in Learning},
  author={Xu, Songlin and Zhang, Xinyu and Qin, Lianhui},
  journal={arXiv preprint arXiv:2404.07963},
  year={2024}
}

@misc{zhang2017dynamickeyvaluememorynetworks,
      title={Dynamic Key-Value Memory Networks for Knowledge Tracing}, 
      author={Jiani Zhang and Xingjian Shi and Irwin King and Dit-Yan Yeung},
      year={2017},
      eprint={1611.08108},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1611.08108}, 
}

@inproceedings{reza2024abscribe,
  title={ABScribe: Rapid Exploration \& Organization of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models},
  author={Reza, Mohi and Laundry, Nathan M and Musabirov, Ilya and Dushniku, Peter and Yu, Zhi Yuan “Michael” and Mittal, Kashish and Grossman, Tovi and Liut, Michael and Kuzminykh, Anastasia and Williams, Joseph Jay},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2024}
}

@inproceedings{stromel2024narrating,
  title={Narrating Fitness: Leveraging Large Language Models for Reflective Fitness Tracker Data Interpretation},
  author={Str{\"o}mel, Konstantin R and Henry, Stanislas and Johansson, Tim and Niess, Jasmin and Wo{\'z}niak, Pawe{\l} W},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--16},
  year={2024}
}

@inproceedings{petridis2023anglekindling,
  title={Anglekindling: Supporting journalistic angle ideation with large language models},
  author={Petridis, Savvas and Diakopoulos, Nicholas and Crowston, Kevin and Hansen, Mark and Henderson, Keren and Jastrzebski, Stan and Nickerson, Jeffrey V and Chilton, Lydia B},
  booktitle={Proceedings of the 2023 CHI conference on human factors in computing systems},
  pages={1--16},
  year={2023}
}

@inproceedings{wang2023enabling,
  title={Enabling conversational interaction with mobile ui using large language models},
  author={Wang, Bryan and Li, Gang and Li, Yang},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2023}
}

@inproceedings{jin2024teach,
  title={Teach AI How to Code: Using Large Language Models as Teachable Agents for Programming Education},
  author={Jin, Hyoungwook and Lee, Seonghee and Shin, Hyungyu and Kim, Juho},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--28},
  year={2024}
}

@inproceedings{wang2023popblends,
  title={PopBlends: Strategies for conceptual blending with large language models},
  author={Wang, Sitong and Petridis, Savvas and Kwon, Taeahn and Ma, Xiaojuan and Chilton, Lydia B},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2023}
}

@inproceedings{suh2024luminate,
  title={Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation},
  author={Suh, Sangho and Chen, Meng and Min, Bryan and Li, Toby Jia-Jun and Xia, Haijun},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--26},
  year={2024}
}

@inproceedings{arakawa2023catalyst,
  title={CatAlyst: domain-extensible intervention for preventing task procrastination using large generative models},
  author={Arakawa, Riku and Yakura, Hiromu and Goto, Masataka},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2023}
}

@inproceedings{bhattacharjee2024understanding,
  title={Understanding the Role of Large Language Models in Personalizing and Scaffolding Strategies to Combat Academic Procrastination},
  author={Bhattacharjee, Ananya and Zeng, Yuchen and Xu, Sarah Yi and Kulzhabayeva, Dana and Ma, Minyi and Kornfield, Rachel and Ahmed, Syed Ishtiaque and Mariakakis, Alex and Czerwinski, Mary P and Kuzminykh, Anastasia and others},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2024}
}

@inproceedings{gao2024collabcoder,
  title={CollabCoder: a lower-barrier, rigorous workflow for inductive collaborative qualitative analysis with large language models},
  author={Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--29},
  year={2024}
}

@inproceedings{rajashekar2024human,
  title={Human-Algorithmic Interaction Using a Large Language Model-Augmented Artificial Intelligence Clinical Decision Support System},
  author={Rajashekar, Niroop Channa and Shin, Yeo Eun and Pu, Yuan and Chung, Sunny and You, Kisung and Giuffre, Mauro and Chan, Colleen E and Saarinen, Theo and Hsiao, Allen and Sekhon, Jasjeet and others},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--20},
  year={2024}
}

@inproceedings{wang2024virtuwander,
  title={Virtuwander: Enhancing multi-modal interaction for virtual tour guidance through large language models},
  author={Wang, Zhan and Yuan, Lin-Ping and Wang, Liangwei and Jiang, Bingchuan and Zeng, Wei},
  booktitle={Proceedings of the CHI conference on human factors in computing systems},
  pages={1--20},
  year={2024}
}

@inproceedings{liu2023wants,
  title={“What it wants me to say”: Bridging the abstraction gap between end-user programmers and code-generating large language models},
  author={Liu, Michael Xieyang and Sarkar, Advait and Negreanu, Carina and Zorn, Benjamin and Williams, Jack and Toronto, Neil and Gordon, Andrew D},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--31},
  year={2023}
}

@inproceedings{wu2022ai,
  title={Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts},
  author={Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
  booktitle={Proceedings of the 2022 CHI conference on human factors in computing systems},
  pages={1--22},
  year={2022}
}

@inproceedings{duan2024generating,
  title={Generating Automatic Feedback on UI Mockups with Large Language Models},
  author={Duan, Peitong and Warner, Jeremy and Li, Yang and Hartmann, Bjoern},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--20},
  year={2024}
}

@article{li2024learning,
  title={Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation},
  author={Li, Qingyao and Xia, Wei and Du, Kounianhua and Zhang, Qiji and Zhang, Weinan and Tang, Ruiming and Yu, Yong},
  journal={arXiv preprint arXiv:2405.12442},
  year={2024}
}

@incollection{vadaparty2024cs1,
  title={CS1-LLM: Integrating LLMs into CS1 Instruction},
  author={Vadaparty, Annapurna and Zingaro, Daniel and Smith IV, David H and Padala, Mounika and Alvarado, Christine and Gorson Benario, Jamie and Porter, Leo},
  booktitle={Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
  pages={297--303},
  year={2024}
}

@article{liang2023let,
  title={Let gpt be a math tutor: Teaching math word problem solvers with customized exercise generation},
  author={Liang, Zhenwen and Yu, Wenhao and Rajpurohit, Tanmay and Clark, Peter and Zhang, Xiangliang and Kaylan, Ashwin},
  journal={arXiv preprint arXiv:2305.14386},
  year={2023}
}

@inproceedings{gebreegziabher2023patat,
  title={Patat: Human-ai collaborative qualitative coding with explainable interactive rule synthesis},
  author={Gebreegziabher, Simret Araya and Zhang, Zheng and Tang, Xiaohang and Meng, Yihao and Glassman, Elena L and Li, Toby Jia-Jun},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2023}
}

@inproceedings{lu2023readingquizmaker,
  title={ReadingQuizMaker: a human-NLP collaborative system that supports instructors to design high-quality reading quiz questions},
  author={Lu, Xinyi and Fan, Simin and Houghton, Jessica and Wang, Lu and Wang, Xu},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2023}
}

@inproceedings{hou2024codetailor,
  title={Codetailor: Llm-powered personalized parsons puzzles for engaging support while learning programming},
  author={Hou, Xinying and Wu, Zihan and Wang, Xu and Ericson, Barbara J},
  booktitle={Proceedings of the Eleventh ACM Conference on Learning@ Scale},
  pages={51--62},
  year={2024}
}

@article{matelsky2023large,
  title={A large language model-assisted education tool to provide feedback on open-ended responses},
  author={Matelsky, Jordan K and Parodi, Felipe and Liu, Tony and Lange, Richard D and Kording, Konrad P},
  journal={arXiv preprint arXiv:2308.02439},
  year={2023}
}

@inproceedings{qin2024charactermeet,
  title={CharacterMeet: Supporting Creative Writers' Entire Story Character Construction Processes Through Conversation with LLM-Powered Chatbot Avatars},
  author={Qin, Hua Xuan and Jin, Shan and Gao, Ze and Fan, Mingming and Hui, Pan},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2024}
}

@inproceedings{wang2024human,
  title={Human-LLM collaborative annotation through effective verification of LLM labels},
  author={Wang, Xinru and Kim, Hannah and Rahman, Sajjadur and Mitra, Kushan and Miao, Zhengjie},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2024}
}

@inproceedings{jo2023understanding,
  title={Understanding the benefits and challenges of deploying conversational AI leveraging large language models for public health intervention},
  author={Jo, Eunkyung and Epstein, Daniel A and Jung, Hyunhoon and Kim, Young-Ho},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--16},
  year={2023}
}

@inproceedings{jo2024understanding,
  title={Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention},
  author={Jo, Eunkyung and Jeong, Yuin and Park, SoHyun and Epstein, Daniel A and Kim, Young-Ho},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2024}
}

@inproceedings{ma2024evaluating,
  title={Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support},
  author={Ma, Zilin and Mei, Yiyang and Long, Yinru and Su, Zhaoyuan and Gajos, Krzysztof Z},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2024}
}

@inproceedings{fok2024marco,
  title={Marco: Supporting Business Document Workflows via Collection-Centric Information Foraging with Large Language Models},
  author={Fok, Raymond and Lipka, Nedim and Sun, Tong and Siu, Alexa F},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--20},
  year={2024}
}

@inproceedings{yang2024reactgenie,
  title={ReactGenie: A Development Framework for Complex Multimodal Interactions Using Large Language Models},
  author={Yang, Jackie and Shi, Yingtian and Zhang, Yuhan and Li, Karina and Rosli, Daniel Wan and Jain, Anisha and Zhang, Shuning and Li, Tianshi and Landay, James A and Lam, Monica S},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--23},
  year={2024}
}

@inproceedings{calle2024towards,
  title={Towards AI-Driven Healthcare: Systematic Optimization, Linguistic Analysis, and Clinicians’ Evaluation of Large Language Models for Smoking Cessation Interventions},
  author={Calle, Paul and Shao, Ruosi and Liu, Yunlong and H{\'e}bert, Emily T and Kendzor, Darla and Neil, Jordan and Businelle, Michael and Pan, Chongle},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--16},
  year={2024}
}

@inproceedings{seo2024chacha,
  title={ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events},
  author={Seo, Woosuk and Yang, Chanmo and Kim, Young-Ho},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--20},
  year={2024}
}


@inproceedings{kim2024mindfuldiary,
  title={MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling},
  author={Kim, Taewan and Bae, Seolyeong and Kim, Hyun Ah and Lee, Su-woo and Hong, Hwajung and Yang, Chanmo and Kim, Young-Ho},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--20},
  year={2024}
}

@inproceedings{hou2024c2ideas,
  title={C2Ideas: Supporting Creative Interior Color Design Ideation with a Large Language Model},
  author={Hou, Yihan and Yang, Manling and Cui, Hao and Wang, Lei and Xu, Jie and Zeng, Wei},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2024}
}

@inproceedings{wu2024mindshift,
  title={MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention},
  author={Wu, Ruolan and Yu, Chun and Pan, Xiaole and Liu, Yujia and Zhang, Ningning and Fu, Yue and Wang, Yuhan and Zheng, Zhi and Chen, Li and Jiang, Qiaolei and others},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--24},
  year={2024}
}

@inproceedings{zulfikar2024memoro,
  title={Memoro: Using Large Language Models to Realize a Concise Interface for Real-Time Memory Augmentation},
  author={Zulfikar, Wazeer Deen and Chan, Samantha and Maes, Pattie},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2024}
}

@inproceedings{jang2024s,
  title={“It’s the only thing I can trust”: Envisioning Large Language Model Use by Autistic Workers for Communication Assistance},
  author={Jang, JiWoong and Moharana, Sanika and Carrington, Patrick and Begel, Andrew},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2024}
}

@inproceedings{zhang2024mathemyths,
  title={Mathemyths: leveraging large language models to teach mathematical language through Child-AI co-creative storytelling},
  author={Zhang, Chao and Liu, Xuechen and Ziska, Katherine and Jeon, Soobin and Yu, Chi-Lin and Xu, Ying},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--23},
  year={2024}
}

@inproceedings{deng2024large,
  title={Large Language Model Powered Agents in the Web},
  author={Deng, Yang and Zhang, An and Lin, Yankai and Chen, Xu and Wen, Ji-Rong and Chua, Tat-Seng},
  booktitle={Companion Proceedings of the ACM on Web Conference 2024},
  pages={1242--1245},
  year={2024}
}

@inproceedings{wan2024building,
  title={Building LLM-based AI Agents in Social Virtual Reality},
  author={Wan, Hongyu and Zhang, Jinda and Suria, Abdulaziz Arif and Yao, Bingsheng and Wang, Dakuo and Coady, Yvonne and Prpa, Mirjana},
  booktitle={Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2024}
}

@inproceedings{wang2024devcoach,
  title={DevCoach: Supporting Students in Learning the Software Development Life Cycle at Scale with Generative Agents},
  author={Wang, Tianjia and Ramanujan, Ramaraja and Lu, Yi and Mao, Chenyu and Chen, Yan and Brown, Chris},
  booktitle={Proceedings of the Eleventh ACM Conference on Learning@ Scale},
  pages={351--355},
  year={2024}
}

@inproceedings{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle={Proceedings of the 36th annual acm symposium on user interface software and technology},
  pages={1--22},
  year={2023}
}

@misc{piech2015deepknowledgetracing,
      title={Deep Knowledge Tracing}, 
      author={Chris Piech and Jonathan Spencer and Jonathan Huang and Surya Ganguli and Mehran Sahami and Leonidas Guibas and Jascha Sohl-Dickstein},
      year={2015},
      eprint={1506.05908},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1506.05908}, 
}

@inproceedings{10.1145/3474085.3475554,
author = {Guo, Xiaopeng and Huang, Zhijie and Gao, Jie and Shang, Mingyu and Shu, Maojing and Sun, Jun},
title = {Enhancing Knowledge Tracing via Adversarial Training},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475554},
doi = {10.1145/3474085.3475554},
abstract = {We study the problem of knowledge tracing (KT) where the goal is to trace the students' knowledge mastery over time so as to make predictions on their future performance. Owing to the good representation capacity of deep neural networks (DNNs), recent advances on KT have increasingly concentrated on exploring DNNs to improve the performance of KT. However, we empirically reveal that the DNNs based KT models may run the risk of overfitting, especially on small datasets, leading to limited generalization. In this paper, by leveraging the current advances in adversarial training (AT), we propose an efficient AT based KT method (ATKT) to enhance KT model's generalization and thus push the limit of KT. Specifically, we first construct adversarial perturbations and add them on the original interaction embeddings as adversarial examples. The original and adversarial examples are further used to jointly train the KT model, forcing it is not only to be robust to the adversarial examples, but also to enhance the generalization over the original ones. To better implement AT, we then present an efficient attentive-LSTM model as KT backbone, where the key is a proposed knowledge hidden state attention module that adaptively aggregates information from previous knowledge hidden states while simultaneously highlighting the importance of current knowledge hidden state to make a more accurate prediction. Extensive experiments on four public benchmark datasets demonstrate that our ATKT achieves new state-of-the-art performance. Code is available at: https://github.com/xiaopengguo/ATKT.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {367–375},
numpages = {9},
keywords = {knowledge tracing, knowledge hidden state attention, adversarial training},
location = {Virtual Event, China},
series = {MM '21}
}

@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2024think,
  title={Think twice before assure: Confidence estimation for large language models through reflection on multiple answers},
  author={Li, Moxin and Wang, Wenjie and Feng, Fuli and Zhu, Fengbin and Wang, Qifan and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2403.09972},
  year={2024}
}

@article{wang2024taste,
  title={TasTe: Teaching Large Language Models to Translate through Self-Reflection},
  author={Wang, Yutong and Zeng, Jiali and Liu, Xuebo and Meng, Fandong and Zhou, Jie and Zhang, Min},
  journal={arXiv preprint arXiv:2406.08434},
  year={2024}
}

@article{hui2024rot,
  title={RoT: Enhancing Large Language Models with Reflection on Search Trees},
  author={Hui, Wenyang and Wang, Yan and Tu, Kewei and Jiang, Chengyue},
  journal={arXiv preprint arXiv:2404.05449},
  year={2024}
}

@inproceedings{kumar2024supporting,
  title={Supporting Self-Reflection at Scale with Large Language Models: Insights from Randomized Field Experiments in Classrooms},
  author={Kumar, Harsh and Xiao, Ruiwei and Lawson, Benjamin and Musabirov, Ilya and Shi, Jiakai and Wang, Xinyuan and Luo, Huayin and Williams, Joseph Jay and Rafferty, Anna N and Stamper, John and others},
  booktitle={Proceedings of the Eleventh ACM Conference on Learning@ Scale},
  pages={86--97},
  year={2024}
}

@article{yan2024mirror,
  title={Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning},
  author={Yan, Hanqi and Zhu, Qinglin and Wang, Xinyu and Gui, Lin and He, Yulan},
  journal={arXiv preprint arXiv:2402.14963},
  year={2024}
}

@article{ji2023towards,
  title={Towards mitigating hallucination in large language models via self-reflection},
  author={Ji, Ziwei and Yu, Tiezheng and Xu, Yan and Lee, Nayeon and Ishii, Etsuko and Fung, Pascale},
  journal={arXiv preprint arXiv:2310.06271},
  year={2023}
}

@inproceedings{10.1145/3394486.3403282,
author = {Ghosh, Aritra and Heffernan, Neil and Lan, Andrew S.},
title = {Context-Aware Attentive Knowledge Tracing},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403282},
doi = {10.1145/3394486.3403282},
abstract = {Knowledge tracing (KT) refers to the problem of predicting future learner performance given their past performance in educational applications. Recent developments in KT using flexible deep neural network-based models excel at this task. However, these models often offer limited interpretability, thus making them insufficient for personalized learning, which requires using interpretable feedback and actionable recommendations to help learners achieve better learning outcomes. In this paper, we propose attentive knowledge tracing (AKT), which couples flexible attention-based neural network models with a series of novel, interpretable model components inspired by cognitive and psychometric models. AKT uses a novel monotonic attention mechanism that relates a learner's future responses to assessment questions to their past responses; attention weights are computed using exponential decay and a context-aware relative distance measure, in addition to the similarity between questions. Moreover, we use the Rasch model to regularize the concept and question embeddings; these embeddings are able to capture individual differences among questions on the same concept without using an excessive number of parameters. We conduct experiments on several real-world benchmark datasets and show that AKT outperforms existing KT methods (by up to $6\%$ in AUC in some cases) on predicting future learner responses. We also conduct several case studies and show that AKT exhibits excellent interpretability and thus has potential for automated feedback and personalization in real-world educational settings.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2330–2339},
numpages = {10},
keywords = {personalized learning, monotonic attention, knowledge tracing, item response theory},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{NEURIPS2022_75ca2b23,
 author = {Liu, Zitao and Liu, Qiongqiong and Chen, Jiahao and Huang, Shuyan and Tang, Jiliang and Luo, Weiqi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {18542--18555},
 publisher = {Curran Associates, Inc.},
 title = {pyKT: A Python Library to Benchmark Deep Learning based Knowledge Tracing Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/75ca2b23d9794f02a92449af65a57556-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{DBLP:conf/iclr/0001L0H023,
  author       = {Zitao Liu and
                  Qiongqiong Liu and
                  Jiahao Chen and
                  Shuyan Huang and
                  Weiqi Luo},
  title        = {simpleKT: {A} Simple But Tough-to-Beat Baseline for Knowledge Tracing},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/forum?id=9HiGqC9C-KA},
  timestamp    = {Wed, 24 Jul 2024 16:50:33 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/0001L0H023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mason2015eye,
  title={Eye-movement modeling of integrative reading of an illustrated text: Effects on processing and learning},
  author={Mason, Lucia and Pluchino, Patrik and Tornatora, Maria Caterina},
  journal={Contemporary Educational Psychology},
  volume={41},
  pages={172--187},
  year={2015},
  publisher={Elsevier}
}

@article{engbert2003microsaccades,
  title={Microsaccades uncover the orientation of covert attention},
  author={Engbert, Ralf and Kliegl, Reinhold},
  journal={Vision research},
  volume={43},
  number={9},
  pages={1035--1045},
  year={2003},
  publisher={Elsevier}
}


@article{berard1994embedding,
  title={Embedding Riemannian manifolds by their heat kernel},
  author={B{\'e}rard, Pierre and Besson, G{\'e}rard and Gallot, Sylvain},
  journal={Geometric \& Functional Analysis GAFA},
  volume={4},
  number={4},
  pages={373--398},
  year={1994},
  publisher={Springer}
}

@article{coifman2005geometric,
  title={Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps},
  author={Coifman, Ronald R and Lafon, Stephane and Lee, Ann B and Maggioni, Mauro and Nadler, Boaz and Warner, Frederick and Zucker, Steven W},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  volume={102},
  number={21},
  pages={7426--7431},
  year={2005},
  publisher={National Acad Sciences}
}

@article{pedersen2017drift,
  title={The drift diffusion model as the choice rule in reinforcement learning},
  author={Pedersen, Mads Lund and Frank, Michael J and Biele, Guido},
  journal={Psychonomic bulletin \& review},
  volume={24},
  number={4},
  pages={1234--1251},
  year={2017},
  publisher={Springer}
}

@article{steyvers2019large,
  title={A large-scale analysis of task switching practice effects across the lifespan},
  author={Steyvers, Mark and Hawkins, Guy E and Karayanidis, Frini and Brown, Scott D},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={36},
  pages={17735--17740},
  year={2019},
  publisher={National Acad Sciences}
}

@article{roseboom2019activity,
  title={Activity in perceptual classification networks as a basis for human subjective time perception},
  author={Roseboom, Warrick and Fountas, Zafeirios and Nikiforou, Kyriacos and Bhowmik, David and Shanahan, Murray and Seth, Anil K},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={1--9},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and brain sciences},
  volume={40},
  year={2017},
  publisher={Cambridge University Press}
}

@article{cichy2019deep,
  title={Deep neural networks as scientific models},
  author={Cichy, Radoslaw M and Kaiser, Daniel},
  journal={Trends in cognitive sciences},
  volume={23},
  number={4},
  pages={305--317},
  year={2019},
  publisher={Elsevier}
}

@article{mehrer2020individual,
  title={Individual differences among deep neural network models},
  author={Mehrer, Johannes and Spoerer, Courtney J and Kriegeskorte, Nikolaus and Kietzmann, Tim C},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{battleday2017modeling,
  title={Modeling human categorization of natural images using deep feature representations},
  author={Battleday, Ruairidh M and Peterson, Joshua C and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:1711.04855},
  year={2017}
}

@article{song2017reward,
  title={Reward-based training of recurrent neural networks for cognitive and value-based tasks},
  author={Song, H Francis and Yang, Guangyu R and Wang, Xiao-Jing},
  journal={Elife},
  volume={6},
  pages={e21492},
  year={2017},
  publisher={eLife Sciences Publications Limited}
}

@article{yang2019task,
  title={Task representations in neural networks trained to perform many cognitive tasks},
  author={Yang, Guangyu Robert and Joglekar, Madhura R and Song, H Francis and Newsome, William T and Wang, Xiao-Jing},
  journal={Nature neuroscience},
  volume={22},
  number={2},
  pages={297--306},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{ratcliff2008diffusion,
  title={The diffusion decision model: theory and data for two-choice decision tasks},
  author={Ratcliff, Roger and McKoon, Gail},
  journal={Neural computation},
  volume={20},
  number={4},
  pages={873--922},
  year={2008},
  publisher={MIT Press}
}

@article{song2016training,
  title={Training excitatory-inhibitory recurrent neural networks for cognitive tasks: a simple and flexible framework},
  author={Song, H Francis and Yang, Guangyu R and Wang, Xiao-Jing},
  journal={PLoS computational biology},
  volume={12},
  number={2},
  pages={e1004792},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{peterson2021using,
  title={Using large-scale experiments and machine learning to discover theories of human decision-making},
  author={Peterson, Joshua C and Bourgin, David D and Agrawal, Mayank and Reichman, Daniel and Griffiths, Thomas L},
  journal={Science},
  volume={372},
  number={6547},
  pages={1209--1214},
  year={2021},
  publisher={American Association for the Advancement of Science}
}

@article{ma2020neural,
  title={A neural network walks into a lab: towards using deep nets as models for human behavior},
  author={Ma, Wei Ji and Peters, Benjamin},
  journal={arXiv preprint arXiv:2005.02181},
  year={2020}
}

@inproceedings{do2021simulation,
  title={A simulation model of intermittently controlled point-and-click behaviour},
  author={Do, Seungwon and Chang, Minsuk and Lee, Byungjoo},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2021}
}

@inproceedings{park2020intermittent,
  title={An intermittent click planning model},
  author={Park, Eunji and Lee, Byungjoo},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2020}
}

@article{de2019overview,
  title={An overview of models for response times and processes in cognitive tests},
  author={De Boeck, Paul and Jeon, Minjeong},
  journal={Frontiers in psychology},
  volume={10},
  pages={102},
  year={2019},
  publisher={Frontiers Media SA}
}

@article{kumbhar2020anytime,
  title={Anytime prediction as a model of human reaction time},
  author={Kumbhar, Omkar and Sizikova, Elena and Majaj, Najib and Pelli, Denis G},
  journal={arXiv preprint arXiv:2011.12859},
  year={2020}
}

@article{noti2016behavior,
  title={Behavior-based machine-learning: A hybrid approach for predicting human decision making},
  author={Noti, Gali and Levi, Effi and Kolumbus, Yoav and Daniely, Amit},
  journal={arXiv preprint arXiv:1611.10228},
  year={2016}
}

@article{battleday2020capturing,
  title={Capturing human categorization of natural images by combining deep networks and cognitive models},
  author={Battleday, Ruairidh M and Peterson, Joshua C and Griffiths, Thomas L},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--14},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{bourgin2019cognitive,
  title={Cognitive model priors for predicting human decisions},
  author={Bourgin, David D and Peterson, Joshua C and Reichman, Daniel and Russell, Stuart J and Griffiths, Thomas L},
  booktitle={International conference on machine learning},
  pages={5133--5141},
  year={2019},
  organization={PMLR}
}

@inproceedings{ritter2017cognitive,
  title={Cognitive psychology for deep neural networks: A shape bias case study},
  author={Ritter, Samuel and Barrett, David GT and Santoro, Adam and Botvinick, Matt M},
  booktitle={International conference on machine learning},
  pages={2940--2949},
  year={2017},
  organization={PMLR}
}

@article{golan2020controversial,
  title={Controversial stimuli: Pitting neural networks against each other as models of human cognition},
  author={Golan, Tal and Raju, Prashant C and Kriegeskorte, Nikolaus},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={47},
  pages={29330--29337},
  year={2020},
  publisher={National Acad Sciences}
}

@article{hartford2016deep,
  title={Deep learning for predicting human strategic behavior},
  author={Hartford, Jason S and Wright, James R and Leyton-Brown, Kevin},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{wenliang2018deep,
  title={Deep neural networks for modeling visual perceptual learning},
  author={Wenliang, Li K and Seitz, Aaron R},
  journal={Journal of Neuroscience},
  volume={38},
  number={27},
  pages={6028--6044},
  year={2018},
  publisher={Soc Neuroscience}
}

@article{singh2020end,
  title={End-to-end deep prototype and exemplar models for predicting human behavior},
  author={Singh, Pulkit and Peterson, Joshua C and Battleday, Ruairidh M and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2007.08723},
  year={2020}
}

@article{peterson2018evaluating,
  title={Evaluating (and improving) the correspondence between deep neural networks and human representations},
  author={Peterson, Joshua C and Abbott, Joshua T and Griffiths, Thomas L},
  journal={Cognitive science},
  volume={42},
  number={8},
  pages={2648--2669},
  year={2018},
  publisher={Wiley Online Library}
}

@article{battleday2021convolutional,
  title={From convolutional neural networks to models of higher-level cognition (and back again)},
  author={Battleday, Ruairidh M and Peterson, Joshua C and Griffiths, Thomas L},
  journal={Annals of the New York Academy of Sciences},
  volume={1505},
  number={1},
  pages={55--78},
  year={2021},
  publisher={Wiley Online Library}
}

@article{viejo2015modeling,
  title={Modeling choice and reaction time during arbitrary visuomotor learning through the coordination of adaptive working memory and reinforcement learning},
  author={Viejo, Guillaume and Khamassi, Mehdi and Brovelli, Andrea and Girard, Beno{\^\i}t},
  journal={Frontiers in behavioral neuroscience},
  volume={9},
  pages={225},
  year={2015},
  publisher={Frontiers Media SA}
}

@inproceedings{plonsky2017psychological,
  title={Psychological forest: Predicting human behavior},
  author={Plonsky, Ori and Erev, Ido and Hazan, Tamir and Tennenholtz, Moshe},
  booktitle={Thirty-first AAAI conference on artificial intelligence},
  year={2017}
}

@article{PEYSAKHOVICH2017373,
title = {Using methods from machine learning to evaluate behavioral models of choice under risk and ambiguity},
journal = {Journal of Economic Behavior \& Organization},
volume = {133},
pages = {373-384},
year = {2017},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2016.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167268116301846},
author = {Alexander Peysakhovich and Jeffrey Naecker},
keywords = {Behavioral economics, Machine learning, Risk, Ambiguity, Decision-making},
abstract = {How can behavioral science incorporate tools from machine learning (ML)? We propose that ML models can be used as upper bounds for the “explainable” variance in a given data set and thus serve as upper bounds for the potential power of a theory. We demonstrate this method in the domain of uncertainty. We ask over 600 individuals to make a total of 6000 choices with randomized parameters and compare standard economic models to ML models. In the domain of risk, a version of expected utility that allows for non-linear probability weighting (as in cumulative prospect theory) and individual-level parameters performs as well out-of-sample as ML techniques. By contrast, in the domain of ambiguity, two of the most widely studied models (a linear version of maximin preferences and second order expected utility) fail to compete with the ML methods. We open the “black boxes” of the ML methods and show that under risk we “rediscover” expected utility with probability weighting. However, in the case of ambiguity the form of ambiguity aversion implied by our ML models suggests that there is gain from theoretical work on a portable model of ambiguity aversion. Our results highlight ways in which behavioral scientists can incorporate ML techniques in their daily practice to gain genuinely new insights.}
}

@article{orhan2017efficient,
  title={Efficient probabilistic inference in generic neural networks trained with non-probabilistic feedback},
  author={Orhan, A Emin and Ma, Wei Ji},
  journal={Nature communications},
  volume={8},
  number={1},
  pages={1--14},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{erev2017anomalies,
  title={From anomalies to forecasts: Toward a descriptive model of decisions under risk, under ambiguity, and from experience.},
  author={Erev, Ido and Ert, Eyal and Plonsky, Ori and Cohen, Doron and Cohen, Oded},
  journal={Psychological review},
  volume={124},
  number={4},
  pages={369},
  year={2017},
  publisher={American Psychological Association}
}

@inproceedings{mickey2014neural,
  title={A neural network model of learning mathematical equivalence},
  author={Mickey, Kevin W and McClelland, James L},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={36},
  number={36},
  year={2014}
}

@article{drori2021neural,
  title={A Neural Network Solves, Explains, and Generates University Math Problems by Program Synthesis and Few-Shot Learning at Human Level},
  author={Drori, Iddo and Zhang, Sarah and Shuttleworth, Reece and Tang, Leonard and Lu, Albert and Ke, Elizabeth and Liu, Kevin and Chen, Linda and Tran, Sunny and Cheng, Newman and others},
  journal={arXiv preprint arXiv:2112.15594},
  year={2021}
}

@article{daitch2016mapping,
  title={Mapping human temporal and parietal neuronal population activity and functional coupling during mathematical cognition},
  author={Daitch, Amy L and Foster, Brett L and Schrouff, Jessica and Rangarajan, Vinitha and Ka{\c{s}}ik{\c{c}}i, It{\i}r and Gattas, Sandra and Parvizi, Josef},
  journal={Proceedings of the National Academy of Sciences},
  volume={113},
  number={46},
  pages={E7277--E7286},
  year={2016},
  publisher={National Acad Sciences}
}

@inproceedings{whittaker2016don,
  title={'Don't Waste My Time' Use of Time Information Improves Focus},
  author={Whittaker, Steve and Kalnikaite, Vaiva and Hollis, Victoria and Guydish, Andrew},
  booktitle={Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
  pages={1729--1738},
  year={2016}
}

@inproceedings{cheng2017evaluation,
  title={Evaluation of effect on cognition response to time pressure by using EEG},
  author={Cheng, Shyh-Yueh},
  booktitle={International conference on applied human factors and ergonomics},
  pages={45--52},
  year={2017},
  organization={Springer}
}

@incollection{edland1993judgment,
  title={Judgment and decision making under time pressure},
  author={Edland, Anne and Svenson, Ola},
  booktitle={Time pressure and stress in human judgment and decision making},
  pages={27--40},
  year={1993},
  publisher={Springer}
}

@incollection{moore2012time,
  title={Time pressure, performance, and productivity},
  author={Moore, Don A and Tenney, Elizabeth R},
  booktitle={Looking back, moving forward: A review of group and team-based research},
  volume={15},
  pages={305--326},
  year={2012},
  publisher={Emerald Group Publishing Limited}
}

@article{zur1981effect,
  title={The effect of time pressure on risky choice behavior},
  author={Zur, Hasida Ben and Breznitz, Shlomo J},
  journal={Acta Psychologica},
  volume={47},
  number={2},
  pages={89--104},
  year={1981},
  publisher={Elsevier}
}

@article{slobounov2000neurophysiological,
  title={Neurophysiological and behavioral indices of time pressure effects on visuomotor task performance},
  author={Slobounov, SM and Fukada, K and Simon, R and Rearick, M and Ray, W},
  journal={Cognitive Brain Research},
  volume={9},
  number={3},
  pages={287--298},
  year={2000},
  publisher={Elsevier}
}

@article{spoerer2020recurrent,
  title={Recurrent neural networks can explain flexible trading of speed and accuracy in biological vision},
  author={Spoerer, Courtney J and Kietzmann, Tim C and Mehrer, Johannes and Charest, Ian and Kriegeskorte, Nikolaus},
  journal={PLoS computational biology},
  volume={16},
  number={10},
  pages={e1008215},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{nguyen2015effectiveness,
  title={The effectiveness of online learning: Beyond no significant difference and future horizons},
  author={Nguyen, Tuan},
  journal={MERLOT Journal of online learning and teaching},
  volume={11},
  number={2},
  pages={309--319},
  year={2015}
}

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{Schulman2017ProximalPO,
  title={Proximal Policy Optimization Algorithms},
  author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.06347}
}

@article{zweifel2021dynamical,
  title={A dynamical model for generating synthetic data to quantify active tactile sensing behavior in the rat},
  author={Zweifel, Nadina O and Bush, Nicholas E and Abraham, Ian and Murphey, Todd D and Hartmann, Mitra JZ},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={27},
  pages={e2011905118},
  year={2021},
  publisher={National Acad Sciences}
}

@inproceedings{corley2020paper,
  title={Paper or ide? the impact of exam format on student performance in a cs1 course},
  author={Corley, Jonathan and Stanescu, Ana and Baumstark, Lewis and Orsega, Michael C},
  booktitle={Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
  pages={706--712},
  year={2020}
}

@article{takano1999psychological,
  title={Psychological biases affecting human cognitive performance in dynamic operational environments},
  author={Takano, Kenichi and Reason, James},
  journal={Journal of Nuclear Science and Technology},
  volume={36},
  number={11},
  pages={1041--1051},
  year={1999},
  publisher={Taylor \& Francis}
}

@article{lin2011spatial,
  title={Spatial and temporal EEG dynamics of dual-task driving performance},
  author={Lin, Chin-Teng and Chen, Shi-An and Chiu, Tien-Ting and Lin, Hong-Zhang and Ko, Li-Wei},
  journal={Journal of neuroengineering and rehabilitation},
  volume={8},
  number={1},
  pages={1--13},
  year={2011},
  publisher={BioMed Central}
}

@article{judd2021training,
  title={Training spatial cognition enhances mathematical learning in a randomized study of 17,000 children},
  author={Judd, Nicholas and Klingberg, Torkel},
  journal={Nature Human Behaviour},
  volume={5},
  number={11},
  pages={1548--1554},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}

@article{Waskom2021,
    doi = {10.21105/joss.03021},
    url = {https://doi.org/10.21105/joss.03021},
    year = {2021},
    publisher = {The Open Journal},
    volume = {6},
    number = {60},
    pages = {3021},
    author = {Michael L. Waskom},
    title = {seaborn: statistical data visualization},
    journal = {Journal of Open Source Software}
 }
 
 @INPROCEEDINGS{9207385,  author={Yanushkevich, S. and Stoica, A. and Shmerko, P. and Howells, W. and Crockett, K. and Guest, R.},  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},   title={Cognitive Identity Management: Synthetic Data, Risk and Trust},   year={2020},  volume={},  number={},  pages={1-8},  doi={10.1109/IJCNN48605.2020.9207385}}
 
@article{de2021next,
  title={Next-generation deep learning based on simulators and synthetic data},
  author={de Melo, Celso M and Torralba, Antonio and Guibas, Leonidas and DiCarlo, James and Chellappa, Rama and Hodgins, Jessica},
  journal={Trends in cognitive sciences},
  year={2021},
  publisher={Elsevier}
}

@article{palminteri2017importance,
  title={The importance of falsification in computational cognitive modeling},
  author={Palminteri, Stefano and Wyart, Valentin and Koechlin, Etienne},
  journal={Trends in cognitive sciences},
  volume={21},
  number={6},
  pages={425--433},
  year={2017},
  publisher={Elsevier}
}

@article{faller2019regulation,
  title={Regulation of arousal via online neurofeedback improves human performance in a demanding sensory-motor task},
  author={Faller, Josef and Cummings, Jennifer and Saproo, Sameer and Sajda, Paul},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={13},
  pages={6482--6490},
  year={2019},
  publisher={National Acad Sciences}
}

@article{fudenberg2020testing,
  title={Testing the drift-diffusion model},
  author={Fudenberg, Drew and Newey, Whitney and Strack, Philipp and Strzalecki, Tomasz},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={52},
  pages={33141--33148},
  year={2020},
  publisher={National Acad Sciences}
}

@article{costa2019boostmeup,
  title={Boostmeup: Improving cognitive performance in the moment by unobtrusively regulating emotions with a smartwatch},
  author={Costa, Jean and Guimbreti{\`e}re, Fran{\c{c}}ois and Jung, Malte F and Choudhury, Tanzeem},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={3},
  number={2},
  pages={1--23},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{schurmann2020personalizing,
  title={Personalizing human-agent interaction through cognitive models},
  author={Sch{\"u}rmann, Tim and Beckerle, Philipp},
  journal={Frontiers in Psychology},
  volume={11},
  pages={561510},
  year={2020},
  publisher={Frontiers Media SA}
}

@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014}
}

@INPROCEEDINGS{8647331,  author={Huang, Chieh-Yang and Ku, Lun-Wei},  booktitle={2018 IEEE Global Communications Conference (GLOBECOM)},   title={EmotionPush: Emotion and Response Time Prediction Towards Human-Like Chatbots},   year={2018},  volume={},  number={},  pages={206-212},  doi={10.1109/GLOCOM.2018.8647331}}

@inproceedings{10.5555/646366.689307,
author = {Han, Jun and Moraga, Claudio},
title = {The Influence of the Sigmoid Function Parameters on the Speed of Backpropagation Learning},
year = {1995},
isbn = {3540594973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the International Workshop on Artificial Neural Networks: From Natural to Artificial Neural Computation},
pages = {195–201},
numpages = {7},
series = {IWANN '96}
}


@inproceedings{ban2015improving,
  title={Improving work productivity by controlling the time rate displayed by the virtual clock},
  author={Ban, Yuki and Sakurai, Sho and Narumi, Takuji and Tanikawa, Tomohiro and Hirose, Michitaka},
  booktitle={Proceedings of the 6th Augmented Human International Conference},
  pages={25--32},
  year={2015}
}


@article{zhai2004speed,
  title={Speed--accuracy tradeoff in Fitts’ law tasks—on the equivalency of actual and nominal pointing precision},
  author={Zhai, Shumin and Kong, Jing and Ren, Xiangshi},
  journal={International journal of human-computer studies},
  volume={61},
  number={6},
  pages={823--856},
  year={2004},
  publisher={Elsevier}
}

@inproceedings{chen2017cognitive,
  title={A cognitive model of how people make decisions through interaction with visual displays},
  author={Chen, Xiuli and Starke, Sandra Dorothee and Baber, Chris and Howes, Andrew},
  booktitle={Proceedings of the 2017 CHI conference on human factors in computing systems},
  pages={1205--1216},
  year={2017}
}

@inproceedings{moon2022speeding,
  title={Speeding up Inference with User Simulators through Policy Modulation},
  author={Moon, Hee-Seung and Do, Seungwon and Kim, Wonjae and Seo, Jiwon and Chang, Minsuk and Lee, Byungjoo},
  booktitle={CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2022}
}

@article{mayer2003causes,
  title={What causes individual differences in cognitive performance},
  author={Mayer, Richard E},
  journal={The psychology of abilities, competencies, and expertise},
  pages={263--273},
  year={2003},
  publisher={Cambridge University Press New York}
}

@article{brice2002effects,
  title={Effects of caffeine on mood and performance: a study of realistic consumption},
  author={Brice, Carolyn F and Smith, Andrew P},
  journal={Psychopharmacology},
  volume={164},
  number={2},
  pages={188--192},
  year={2002},
  publisher={Springer}
}

@article{childs2014influence,
  title={Influence of energy drink ingredients on mood and cognitive performance},
  author={Childs, Emma},
  journal={Nutrition reviews},
  volume={72},
  number={suppl\_1},
  pages={48--59},
  year={2014},
  publisher={Oxford University Press}
}


@inproceedings{warnell2018deep,
  title={Deep tamer: Interactive agent shaping in high-dimensional state spaces},
  author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{arumugam2019deep,
  title={Deep reinforcement learning from policy-dependent human feedback},
  author={Arumugam, Dilip and Lee, Jun Ki and Saskin, Sophie and Littman, Michael L},
  journal={arXiv preprint arXiv:1902.04257},
  year={2019}
}

@inproceedings{10.1145/2207676.2207679,
author = {Szafir, Daniel and Mutlu, Bilge},
title = {Pay Attention! Designing Adaptive Agents That Monitor and Improve User Engagement},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2207679},
doi = {10.1145/2207676.2207679},
abstract = {Embodied agents hold great promise as educational assistants, exercise coaches, and team members in collaborative work. These roles require agents to closely monitor the behavioral, emotional, and mental states of their users and provide appropriate, effective responses. Educational agents, for example, will have to monitor student attention and seek to improve it when student engagement decreases. In this paper, we draw on techniques from brain-computer interfaces (BCI) and knowledge from educational psychology to design adaptive agents that monitor student attention in real time using measurements from electroencephalography (EEG) and recapture diminishing attention levels using verbal and nonverbal cues. An experimental evaluation of our approach showed that an adaptive robotic agent employing behavioral techniques to regain attention during drops in engagement improved student recall abilities 43% over the baseline regardless of student gender and significantly improved female motivation and rapport. Our findings offer guidelines for developing effective adaptive agents, particularly for educational settings.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {11–20},
numpages = {10},
keywords = {passive brain-computer interfaces (bci), educational agents, immediacy, adaptive agents, human-robot interaction, electroencephalography (eeg)},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@article{muller2011drugs,
  title={Drugs as instruments: a new framework for non-addictive psychoactive drug use},
  author={M{\"u}ller, Christian P and Schumann, Gunter},
  journal={Behavioral and Brain Sciences},
  volume={34},
  number={6},
  pages={293},
  year={2011},
  publisher={Cambridge University Press}
}

@article{butt2011coffee,
  title={Coffee and its consumption: benefits and risks},
  author={Butt, Masood Sadiq and Sultan, M Tauseef},
  journal={Critical reviews in food science and nutrition},
  volume={51},
  number={4},
  pages={363--373},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{steinerman2010minding,
  title={Minding the aging brain: technology-enabled cognitive training for healthy elders},
  author={Steinerman, Joshua R},
  journal={Current Neurology and neuroscience reports},
  volume={10},
  number={5},
  pages={374--380},
  year={2010},
  publisher={Springer}
}



@inproceedings{10.1145/3447548.3467181,
author = {Hao, Qianyue and Xu, Fengli and Chen, Lin and Hui, Pan and Li, Yong},
title = {Hierarchical Reinforcement Learning for Scarce Medical Resource Allocation with Imperfect Information},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467181},
doi = {10.1145/3447548.3467181},
abstract = {Facing the outbreak of COVID-19, shortage in medical resources becomes increasingly outstanding. Therefore, efficient strategies for medical resource allocation are urgently called for. Reinforcement learning (RL) is powerful for decision making, but three key challenges exist in solving this problem via RL: (1) complex situation and countless choices for decision making in the real world; (2) only imperfect information are available due to the latency of pandemic spreading; (3) limitations on conducting experiments in real world since we cannot set pandemic outbreaks arbitrarily. In this paper, we propose a hierarchical reinforcement learning method with a corresponding training algorithm. We design a decomposed action space to deal with the countless choices to ensure efficient and real time strategies. We also design a recurrent neural network based framework to utilize the imperfect information obtained from the environment. We build a pandemic spreading simulator based on real world data, serving as the experimental platform. We conduct extensive experiments and the results show that our method outperforms all the baselines, which reduces infections and deaths by 14.25\% on average.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2955–2963},
numpages = {9},
keywords = {hierarchical reinforcement learning, COVID-19 pandemic, imperfect information, medical resource allocation},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@article{10.1145/3527448,
author = {Vouros, George A.},
title = {Explainable Deep Reinforcement Learning: State of the Art and Challenges},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3527448},
doi = {10.1145/3527448},
abstract = {Interpretability, explainability, and transparency are key issues to introducing artificial intelligence methods in many critical domains. This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability, and fairness, and has important consequences toward keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. Although the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article, we aim to provide a review of state-of-the-art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators—that is, of those who make the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state-of-the-art methods, categorizing them into classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes by identifying open questions and important challenges.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {92},
numpages = {39},
keywords = {Deep learning, deep reinforcement learning, transparency, interpretability, explainability}
}

@inproceedings{kosmyna2018attentivu,
  title={AttentivU: Evaluating the feasibility of biofeedback glasses to monitor and improve attention},
  author={Kosmyna, Nataliya and Sarawgi, Utkarsh and Maes, Pattie},
  booktitle={Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers},
  pages={999--1005},
  year={2018}
}

@article{kosmyna2019attentivu,
  title={Attentivu: An EEG-based closed-loop biofeedback system for real-time monitoring and improvement of engagement for personalized learning},
  author={Kosmyna, Nataliya and Maes, Pattie},
  journal={Sensors},
  volume={19},
  number={23},
  pages={5200},
  year={2019},
  publisher={MDPI}
}

@article{silva2013benefits,
  title={Benefits of SenseCam review on neuropsychological test performance},
  author={Silva, Ana R and Pinho, Salom{\'e} and Macedo, Lu{\'\i}s M and Moulin, Chris J},
  journal={American journal of preventive medicine},
  volume={44},
  number={3},
  pages={302--307},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{corey2003memory,
  title={The memory glasses: subliminal vs. overt memory support with imperfect information},
  author={Corey, Vicka R},
  booktitle={Proceedings of the Seventh IEEE International Symposium on Wearable Computers (ISWC’03)},
  volume={1530},
  number={0811/03},
  pages={17--00},
  year={2003},
  organization={Citeseer}
}

@inproceedings{costa2016emotioncheck,
  title={EmotionCheck: leveraging bodily signals and false feedback to regulate our emotions},
  author={Costa, Jean and Adams, Alexander T and Jung, Malte F and Guimbreti{\`e}re, Fran{\c{c}}ois and Choudhury, Tanzeem},
  booktitle={Proceedings of the 2016 ACM international joint conference on pervasive and ubiquitous computing},
  pages={758--769},
  year={2016}
}

@article{iodice2019interoceptive,
  title={An interoceptive illusion of effort induced by false heart-rate feedback},
  author={Iodice, Pierpaolo and Porciello, Giuseppina and Bufalari, Ilaria and Barca, Laura and Pezzulo, Giovanni},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={28},
  pages={13897--13902},
  year={2019},
  publisher={National Acad Sciences}
}

@inproceedings{bassen2020reinforcement,
  title={Reinforcement learning for the adaptive scheduling of educational activities},
  author={Bassen, Jonathan and Balaji, Bharathan and Schaarschmidt, Michael and Thille, Candace and Painter, Jay and Zimmaro, Dawn and Games, Alex and Fast, Ethan and Mitchell, John C},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--12},
  year={2020}
}

@article{liao2020personalized,
  title={Personalized heartsteps: A reinforcement learning algorithm for optimizing physical activity},
  author={Liao, Peng and Greenewald, Kristjan and Klasnja, Predrag and Murphy, Susan},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={4},
  number={1},
  pages={1--22},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{scurto2021designing,
  title={Designing deep reinforcement learning for human parameter exploration},
  author={Scurto, Hugo and Kerrebroeck, Bavo Van and Caramiaux, Baptiste and Bevilacqua, Fr{\'e}d{\'e}ric},
  journal={ACM Transactions on Computer-Human Interaction (TOCHI)},
  volume={28},
  number={1},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}



@misc{hbr,
  author = {David De Cremer and Garry Kasparov},
  title = {AI Should Augment Human Intelligence, Not Replace It},
  year = {2021},
  url = {https://hbr.org/2021/03/ai-should-augment-human-intelligence-not-replace-it},
  lastaccessed = {january 20, 2023},
}

@article{xu2023modeling,
  title={Modeling Human Cognition with a Hybrid Deep Reinforcement Learning Agent},
  author={Xu, Songlin and Zhang, Xinyu},
  journal={arXiv preprint arXiv:2301.06216},
  year={2023}
}



@article{spielberger1971state,
  title={The state-trait anxiety inventory},
  author={Spielberger, Charles D and Gonzalez-Reigosa, Fernando and Martinez-Urrutia, Angel and Natalicio, Luiz FS and Natalicio, Diana S},
  journal={Revista Interamericana de Psicologia/Interamerican Journal of Psychology},
  volume={5},
  number={3 \& 4},
  year={1971}
}

@article{shammas2019simple,
  title={Why a Simple Time-Management System Can Revolutionize How You Work—and Live},
  author={Shammas, Michael},
  journal={Shammas, Michael." Why a Simple Time-Management System Can Revolutionize How You Work—And Live." Medium},
  year={2019}
}

@inproceedings{maclean2013moodwings,
  title={MoodWings: a wearable biofeedback device for real-time stress intervention},
  author={MacLean, Diana and Roseway, Asta and Czerwinski, Mary},
  booktitle={Proceedings of the 6th international conference on PErvasive Technologies Related to Assistive Environments},
  pages={1--8},
  year={2013}
}

@article{mirjafari2021predicting,
  title={Predicting Job Performance Using Mobile Sensing},
  author={Mirjafari, Shayan and Bagherinezhad, Hessam and Nepal, Subigya and Martinez, Gonzalo J and Saha, Koustuv and Obuchi, Mikio and Audia, Pino G and Chawla, Nitesh V and Dey, Anind K and Striegel, Aaron and others},
  journal={IEEE Pervasive Computing},
  volume={20},
  number={4},
  pages={43--51},
  year={2021},
  publisher={IEEE}
}

@inproceedings{10.1145/3055635.3056616,
author = {Tatsuta, Riki and Phuong, Dinh Thi Dong and Kajiwara, Yusuke and Shimakawa, Hiromitsu},
title = {Guidance of Farming Works to Improve Efficiency Considering Physical Behavior},
year = {2017},
isbn = {9781450348171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055635.3056616},
doi = {10.1145/3055635.3056616},
abstract = {New farmers need technical guidance to improve working efficiency because they are lacking in experience. Agricultural experts put much effort to provide guidance for beginner farmers. However, continuing to give guidance is difficult because it is a large burden on the experts. This study proposes a system which contributes to transferring a deft motion of experts to improve the working efficiency of beginners in farm works. The system promotes beginners to assess their own farming works without an expert. The beginners can confirm whether their own works are proper works. An experiment has suggested that machine learning can achieve judgement of the properness of farming works using state transition probability of each body part.},
booktitle = {Proceedings of the 9th International Conference on Machine Learning and Computing},
pages = {28–32},
numpages = {5},
keywords = {Agriculture, machine learning, farming works, acceleration sensor, physical behavior, wearable sensor},
location = {Singapore, Singapore},
series = {ICMLC 2017}
}

@inproceedings{10.1145/2037373.2037501,
author = {Meschtscherjakov, Alexander and Moser, Christiane and Tscheligi, Manfred and Reponen, Erika},
title = {Mobile Work Efficiency: Enhancing Workflows with Mobile Devices},
year = {2011},
isbn = {9781450305419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037373.2037501},
doi = {10.1145/2037373.2037501},
abstract = {This workshop is a forum of multi-disciplinary discussion on how mobile devices can increase perceived work efficiency (PWE), as well as how this subjective enhancement can be measured. It brings together practitioners and researchers from different domains interested in researching perceived workflow efficiency in the mobile context. The overall aim is to create a common base, as well as further extend the research agenda for work efficiency enhancement with the assistance of mobile devices both from a scientific, as well as from an industrial perspective.},
booktitle = {Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services},
pages = {737–740},
numpages = {4},
keywords = {mobile evaluation, work efficiency enhancement},
location = {Stockholm, Sweden},
series = {MobileHCI '11}
}

@article{pruettikomon2018study,
  title={A study and development of workplace facilities and working environment to increase the work efficiency of persons with disabilities: A Case Study Of Major Retail And Wholesale Companies in Bangkok},
  author={Pruettikomon, Soraj and Louhapensang, Chaturong},
  journal={The Scientific World Journal},
  volume={2018},
  year={2018},
  publisher={Hindawi}
}

@inproceedings{10.1145/3411764.3445711,
author = {Ahuja, Karan and Shah, Deval and Pareddy, Sujeath and Xhakaj, Franceska and Ogan, Amy and Agarwal, Yuvraj and Harrison, Chris},
title = {Classroom Digital Twins with Instrumentation-Free Gaze Tracking},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445711},
doi = {10.1145/3411764.3445711},
abstract = {Classroom sensing is an important and active area of research with great potential to improve instruction. Complementing professional observers – the current best practice – automated pedagogical professional development systems can attend every class and capture fine-grained details of all occupants. One particularly valuable facet to capture is class gaze behavior. For students, certain gaze patterns have been shown to correlate with interest in the material, while for instructors, student-centered gaze patterns have been shown to increase approachability and immediacy. Unfortunately, prior classroom gaze-sensing systems have limited accuracy and often require specialized external or worn sensors. In this work, we developed a new computer-vision-driven system that powers a 3D “digital twin” of the classroom and enables whole-class, 6DOF head gaze vector estimation without instrumenting any of the occupants. We describe our open source implementation, and results from both controlled studies and real-world classroom deployments.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {484},
numpages = {9},
keywords = {Classroom sensing, digital twins., gaze tracking},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3351229,
author = {Ahuja, Karan and Kim, Dohyun and Xhakaj, Franceska and Varga, Virag and Xie, Anne and Zhang, Stanley and Townsend, Jay Eric and Harrison, Chris and Ogan, Amy and Agarwal, Yuvraj},
title = {EduSense: Practical Classroom Sensing at Scale},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351229},
doi = {10.1145/3351229},
abstract = {Providing university teachers with high-quality opportunities for professional development cannot happen without data about the classroom environment. Currently, the most effective mechanism is for an expert to observe one or more lectures and provide personalized formative feedback to the instructor. Of course, this is expensive and unscalable, and perhaps most critically, precludes a continuous learning feedback loop for the instructor. In this paper, we present the culmination of two years of research and development on EduSense, a comprehensive sensing system that produces a plethora of theoretically-motivated visual and audio features correlated with effective instruction, which could feed professional development tools in much the same way as a Fitbit sensor reports step count to an end user app. Although previous systems have demonstrated some of our features in isolation, EduSense is the first to unify them into a cohesive, real-time, in-the-wild evaluated, and practically-deployable system. Our two studies quantify where contemporary machine learning techniques are robust, and where they fall short, illuminating where future work remains to bring the vision of automated classroom analytics to reality.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {71},
numpages = {26},
keywords = {Audio, Sensing, Pedagogy, Computer Vision, Teacher, Classroom, Speech Detection, Machine Learning, Instructor}
}

@article{10.1145/3411813,
author = {Gao, Nan and Shao, Wei and Rahaman, Mohammad Saiedur and Salim, Flora D.},
title = {N-Gage: Predicting in-Class Emotional, Behavioural and Cognitive Engagement in the Wild},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3411813},
doi = {10.1145/3411813},
abstract = {The study of student engagement has attracted growing interests to address problems such as low academic performance, disaffection, and high dropout rates. Existing approaches to measuring student engagement typically rely on survey-based instruments. While effective, those approaches are time-consuming and labour-intensive. Meanwhile, both the response rate and quality of the survey are usually poor. As an alternative, in this paper, we investigate whether we can infer and predict engagement at multiple dimensions, just using sensors. We hypothesize that multidimensional student engagement level can be translated into physiological responses and activity changes during the class, and also be affected by the environmental changes. Therefore, we aim to explore the following questions: Can we measure the multiple dimensions of high school student's learning engagement including emotional, behavioural and cognitive engagement with sensing data in the wild? Can we derive the activity, physiological, and environmental factors contributing to the different dimensions of student learning engagement? If yes, which sensors are the most useful in differentiating each dimension of the engagement? Then, we conduct an in-situ study in a high school from 23 students and 6 teachers in 144 classes over 11 courses for 4 weeks. We present the n-Gage, a student engagement sensing system using a combination of sensors from wearables and environments to automatically detect student in-class multidimensional learning engagement. Extensive experiment results show that n-Gage can accurately predict multidimensional student engagement in real-world scenarios with an average mean absolute error (MAE) of 0.788 and root mean square error (RMSE) of 0.975 using all the sensors. We also show a set of interesting findings of how different factors (e.g., combinations of sensors, school subjects, CO2 level) affect each dimension of the student learning engagement.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {79},
numpages = {26},
keywords = {Wearable, Students, Engagement Prediction, Electrodermal Activity}
}

@inproceedings{10.1145/3159450.3159500,
author = {Nip, Terence and Gunter, Elsa L. and Herman, Geoffrey L. and Morphew, Jason W. and West, Matthew},
title = {Using a Computer-Based Testing Facility to Improve Student Learning in a Programming Languages and Compilers Course},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3159500},
doi = {10.1145/3159450.3159500},
abstract = {While most efforts to improve students' learning in computer science education have focused on designing new pedagogies or tools, comparatively little research has focused on redesigning examinations to improve students' learning. Cognitive science research, however, has robustly demonstrated that getting students to practice using their knowledge in testing environments can significantly improve learning through a phenomenon known as the testing effect. The testing effect has been shown to improve learning more than rehearsal strategies such as re-reading a textbook or re-watching lectures. In this paper, we present a quasi-experimental study to examine the effect of using frequent, automated examinations in an advanced computer science course, "Programming Languages and Compilers" (CS 421). In Fall 2014, students were given traditional paper-based exams, but in Fall 2015 a computer-based testing facility enabled the course to offer more frequent examinations while other aspects of the course were held constant. A comparison of 292 student scores across the two semesters revealed a significant change in the distribution of students' grades with fewer students failing the final examination, and proportionately more students now earning grades of B and C instead. This data suggests that focusing on redesigning the nature of examinations may indeed be a relatively untapped opportunity to improve students' learning.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {568–573},
numpages = {6},
keywords = {programming languages, computer-based testing, compilers, testing effect},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@article{10.5555/1231091.1231096,
author = {Nicholson, Darren and Hamilton, Diane and McFarland, Daniel},
title = {Leveraging Learning Styles to Improve Student Learning: The Interactive Learning Model and Learning Combination Inventory},
year = {2007},
issue_date = {June 2007},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {22},
number = {6},
issn = {1937-4771},
abstract = {This paper describes the Interactive Learning Model, and a related instrument called the Learning Combination Inventory, which identifies those patterns of processing that learners rely upon as well as those patterns that learners avoid. It further reports the results of an exploratory study using MIS students, the purpose of which was to see if a correlation exists between those learning patterns that a student relies upon and their success in two different types of learning activities. The results suggest that if students understood their own learning patterns they could develop specific strategies to become efficient and effective life-long learners.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {8–17},
numpages = {10}
}

@inproceedings{10.1145/3084381.3084434,
author = {Panigrahi, Ritanjali},
title = {Online Learning: Improving the Learning Outcomes},
year = {2017},
isbn = {9781450350372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084381.3084434},
doi = {10.1145/3084381.3084434},
abstract = {The use of Technology to facilitate better learning and training is gaining momentum worldwide, reducing the temporal and spatial problems associated with traditional learning. Despite its several benefits, retaining students in online platforms is challenging. This paper discusses an integration of online learning with virtual communities and mobile platforms to foster student engagement for obtaining better learning outcomes in blended learning platforms as well as to understand the public sentiments regarding the pure online learning platforms and finding the factors to improve the learning outcome. The potential utility of the study is discussed in terms of the contribution to the theory and practice.},
booktitle = {Proceedings of the 2017 ACM SIGMIS Conference on Computers and People Research},
pages = {203–204},
numpages = {2},
keywords = {virtual communities, online learning, moocs, mobile technologies, learning outcomes},
location = {Bangalore, India},
series = {SIGMIS-CPR '17}
}

@inproceedings{10.1145/3328778.3366880,
author = {Baham\'{o}n, Julio C\'{e}sar and Rorrer, Audrey},
title = {Improving Student Learning Outcomes in Online Courses: An Investigation Into the Effects of Multiple Teaching Modalities},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366880},
doi = {10.1145/3328778.3366880},
abstract = {We investigated learning outcomes within an undergraduate C Programming course that is taught in multiple modalities: in-person, online and blended. Because this course has been taught by the same instructor, using the same scaffolding activities, materials and approaches, we were uniquely positioned to conduct a quasi-experimental study of learning outcomes between courses and within students. The overarching goal was to glean knowledge and implications about assessment practices for undergraduate courses that are taught in multiple modalities. The objectives of our research are primarily to discern what differential impacts, if any, are found between the in-person and the online course delivery. We aimed to discover learning outcome patterns among the students who participate in these modalities. Findings from this study provide valuable information for undergraduate Computer Science programs by identifying any differential learning outcomes that students experience between in-person and online course instruction. The research questions addressed by the study were as follows: 1) What impact does modality have on student learning outcomes? 2) What patterns are discernable across student groups? 3) What relationship is there between final course grades and assignment module learning outcomes? In earlier work, we were surprised that no significant differences were obtained between course modality. While this was an encouraging finding, we believed that further data collection and analysis were needed, before making general conclusions about the two modalities. This paper reports on our efforts to collect additional data, while considering additional variables, such as instructor, multiple modalities, and online course design approaches.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1179–1185},
numpages = {7},
keywords = {retention, race, online education, gender, sex, programming, course modality},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

@article{DBLP:journals/corr/abs-1810-04805,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Gervet2020WhenID,
  title={When is Deep Learning the Best Approach to Knowledge Tracing},
  author={Th{\'e}ophile Gervet and K. Koedinger and Jeff G. Schneider and Tom M. Mitchell},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:226253566}
}

@article{Xu2024ASO,
  title={A Survey on Knowledge Distillation of Large Language Models},
  author={Xiaohan Xu and Ming Li and Chongyang Tao and Tao Shen and Reynold Cheng and Jinyang Li and Can Xu and Dacheng Tao and Tianyi Zhou},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.13116},
  url={https://api.semanticscholar.org/CorpusID:267760021}
}

@article{li2024explainable,
  title={Explainable Few-shot Knowledge Tracing},
  author={Li, Haoxuan and Yu, Jifan and Ouyang, Yuanxin and Liu, Zhuang and Rong, Wenge and Li, Juanzi and Xiong, Zhang},
  journal={arXiv preprint arXiv:2405.14391},
  year={2024}
}

@inproceedings{10.5555/3600270.3601883,
author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
title = {Large language models are zero-shot reasoners},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large-scale InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1613},
numpages = {15},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@inproceedings{
gu2024minillm,
title={Mini{LLM}: Knowledge Distillation of Large Language Models},
author={Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=5h0qf7IBZZ}
}

@misc{neshaei2024modelinglearnerperformancelarge,
      title={Towards Modeling Learner Performance with Large Language Models}, 
      author={Seyed Parsa Neshaei and Richard Lee Davis and Adam Hazimeh and Bojan Lazarevski and Pierre Dillenbourg and Tanja Käser},
      year={2024},
      eprint={2403.14661},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2403.14661}, 
}

@misc{fu2024sinktstructureawareinductiveknowledge,
      title={SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model}, 
      author={Lingyue Fu and Hao Guan and Kounianhua Du and Jianghao Lin and Wei Xia and Weinan Zhang and Ruiming Tang and Yasheng Wang and Yong Yu},
      year={2024},
      eprint={2407.01245},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.01245}, 
}

@misc{lee2024languagemodelknowledgetracing,
      title={Language Model Can Do Knowledge Tracing: Simple but Effective Method to Integrate Language Model and Knowledge Tracing Task}, 
      author={Unggi Lee and Jiyeong Bae and Dohee Kim and Sookbun Lee and Jaekwon Park and Taekyung Ahn and Gunho Lee and Damji Stratton and Hyeoncheol Kim},
      year={2024},
      eprint={2406.02893},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.02893}, 
}

@misc{ouyang2024structuredchemistryreasoninglarge,
      title={Structured Chemistry Reasoning with Large Language Models}, 
      author={Siru Ouyang and Zhuosheng Zhang and Bing Yan and Xuan Liu and Yejin Choi and Jiawei Han and Lianhui Qin},
      year={2024},
      eprint={2311.09656},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09656}, 
}

@misc{wang2023understandingchainofthoughtpromptingempirical,
      title={Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters}, 
      author={Boshi Wang and Sewon Min and Xiang Deng and Jiaming Shen and You Wu and Luke Zettlemoyer and Huan Sun},
      year={2023},
      eprint={2212.10001},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.10001}, 
}

@misc{zhang2024questioncentricmultiexpertscontrastivelearning,
      title={A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models}, 
      author={Hengyuan Zhang and Zitao Liu and Chenming Shang and Dawei Li and Yong Jiang},
      year={2024},
      eprint={2403.07322},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2403.07322}, 
}

@misc{jung2024clstcoldstartmitigationknowledge,
      title={CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a Generative Language Model as a Students' Knowledge Tracer}, 
      author={Heeseok Jung and Jaesang Yoo and Yohaan Yoon and Yeonju Jang},
      year={2024},
      eprint={2406.10296},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10296}, 
}

@misc{zhang2024improvinglowresourceknowledgetracing,
      title={Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning}, 
      author={Hengyuan Zhang and Zitao Liu and Shuyan Huang and Chenming Shang and Bojun Zhan and Yong Jiang},
      year={2024},
      eprint={2403.06725},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2403.06725}, 
}

@article{cui2023adaptive,
  title={Adaptive and personalized exercise generation for online language learning},
  author={Cui, Peng and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2306.02457},
  year={2023}
}

@article{lee2024language,
  title={Language Model Can Do Knowledge Tracing: Simple but Effective Method to Integrate Language Model and Knowledge Tracing Task},
  author={Lee, Unggi and Bae, Jiyeong and Kim, Dohee and Lee, Sookbun and Park, Jaekwon and Ahn, Taekyung and Lee, Gunho and Stratton, Damji and Kim, Hyeoncheol},
  journal={arXiv preprint arXiv:2406.02893},
  year={2024}
}

@inproceedings{lee2024monacobert,
  title={Monacobert: Monotonic attention based convbert for knowledge tracing},
  author={Lee, Unggi and Park, Yonghyun and Kim, Yujin and Choi, Seongyune and Kim, Hyeoncheol},
  booktitle={International Conference on Intelligent Tutoring Systems},
  pages={107--123},
  year={2024},
  organization={Springer}
}

@article{li2024integrating,
  title={Integrating lstm and bert for long-sequence data analysis in intelligent tutoring systems},
  author={Li, Zhaoxing and Yang, Jujie and Wang, Jindi and Shi, Lei and Stein, Sebastian},
  journal={arXiv preprint arXiv:2405.05136},
  year={2024}
}

@inproceedings{zhang2024predicting,
  title={Predicting Learning Performance with Large Language Models: A Study in Adult Literacy},
  author={Zhang, Liang and Lin, Jionghao and Borchers, Conrad and Sabatini, John and Hollander, John and Cao, Meng and Hu, Xiangen},
  booktitle={International Conference on Human-Computer Interaction},
  pages={333--353},
  year={2024},
  organization={Springer}
}

@article{jung2024clst,
  title={CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a Generative Language Model as a Students' Knowledge Tracer},
  author={Jung, Heeseok and Yoo, Jaesang and Yoon, Yohaan and Jang, Yeonju},
  journal={arXiv preprint arXiv:2406.10296},
  year={2024}
}

@article{chen2023agentverse,
  title={Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents},
  author={Chen, Weize and Su, Yusheng and Zuo, Jingwei and Yang, Cheng and Yuan, Chenfei and Qian, Chen and Chan, Chi-Min and Qin, Yujia and Lu, Yaxi and Xie, Ruobing and others},
  journal={arXiv preprint arXiv:2308.10848},
  volume={2},
  number={4},
  pages={6},
  year={2023}
}

@misc{zhou2024predictivescalableinterpretableknowledge,
      title={Predictive, scalable and interpretable knowledge tracing on structured domains}, 
      author={Hanqi Zhou and Robert Bamler and Charley M. Wu and Álvaro Tejero-Cantero},
      year={2024},
      eprint={2403.13179},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.13179}, 
}

@inproceedings{Lu_2024, series={L@S ’24},
   title={Generative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation},
   volume={8},
   url={http://dx.doi.org/10.1145/3657604.3662031},
   DOI={10.1145/3657604.3662031},
   booktitle={Proceedings of the Eleventh ACM Conference on Learning @ Scale},
   publisher={ACM},
   author={Lu, Xinyi and Wang, Xu},
   year={2024},
   month=jul, pages={16–27},
   collection={L@S ’24} 
}



@article{10.1016/j.eswa.2022.117681,
author = {Wu, Zhengyang and Huang, Li and Huang, Qionghao and Huang, Changqin and Tang, Yong},
title = {SGKT: Session graph-based knowledge tracing for student performance prediction},
year = {2022},
issue_date = {Nov 2022},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {206},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2022.117681},
doi = {10.1016/j.eswa.2022.117681},
journal = {Expert Syst. Appl.},
month = {nov},
numpages = {12},
keywords = {Attention Mechanism, Gated Graph Neural Networks, Graph Convolutional Network, Knowledge tracing}
}

@misc{liu2023simplektsimpletoughtobeatbaseline,
      title={simpleKT: A Simple But Tough-to-Beat Baseline for Knowledge Tracing}, 
      author={Zitao Liu and Qiongqiong Liu and Jiahao Chen and Shuyan Huang and Weiqi Luo},
      year={2023},
      eprint={2302.06881},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.06881}, 
}

@inproceedings{Hou_2024, series={L@S ’24},
   title={CodeTailor: LLM-Powered Personalized Parsons Puzzles for Engaging Support While Learning Programming},
   volume={25},
   url={http://dx.doi.org/10.1145/3657604.3662032},
   DOI={10.1145/3657604.3662032},
   booktitle={Proceedings of the Eleventh ACM Conference on Learning @ Scale},
   publisher={ACM},
   author={Hou, Xinying and Wu, Zihan and Wang, Xu and Ericson, Barbara J.},
   year={2024},
   month=jul, pages={51–62},
   collection={L@S ’24} }

@inproceedings{nakagawa2019graph,
  title={Graph-based knowledge tracing: modeling student proficiency using graph neural network},
  author={Nakagawa, Hiromi and Iwasawa, Yusuke and Matsuo, Yutaka},
  booktitle={IEEE/WIC/ACM International Conference on Web Intelligence},
  pages={156--163},
  year={2019}
}

@inproceedings{yudelson2013individualized,
  title={Individualized bayesian knowledge tracing models},
  author={Yudelson, Michael V and Koedinger, Kenneth R and Gordon, Geoffrey J},
  booktitle={Artificial Intelligence in Education: 16th International Conference, AIED 2013, Memphis, TN, USA, July 9-13, 2013. Proceedings 16},
  pages={171--180},
  year={2013},
  organization={Springer}
}

@article{abdelrahman2023knowledge,
  title={Knowledge tracing: A survey},
  author={Abdelrahman, Ghodai and Wang, Qing and Nunes, Bernardo},
  journal={ACM Computing Surveys},
  volume={55},
  number={11},
  pages={1--37},
  year={2023},
  publisher={ACM New York, NY}
}

@article{graesser2012intelligent,
  title={Intelligent tutoring systems.},
  author={Graesser, Arthur C and Conley, Mark W and Olney, Andrew},
  year={2012},
  publisher={American Psychological Association}
}

@article{BKT,
  title={Knowledge tracing: Modeling the acquisition of procedural knowledge},
  author={Albert T. Corbett and John R. Anderson},
  journal={User Modeling and User-Adapted Interaction},
  year={1994},
  volume={4},
}

@INPROCEEDINGS{GKT,
  author={Nakagawa, Hiromi and Iwasawa, Yusuke and Matsuo, Yutaka},
  booktitle={2019 IEEE/WIC/ACM International Conference on Web Intelligence (WI)}, 
  title={Graph-based Knowledge Tracing: Modeling Student Proficiency Using Graph Neural Network}, 
  year={2019},
  volume={},
  number={},
  pages={156-163},
  keywords={Neural networks;Predictive models;Knowledge engineering;Learning systems;Data models;Computational modeling;Data structures;Knowledge tracing;Graph neural network;Educational data mining;Learning sciences},
  doi={}}

@misc{pandey2019selfattentivemodelknowledgetracing,
      title={A Self-Attentive model for Knowledge Tracing}, 
      author={Shalini Pandey and George Karypis},
      year={2019},
      eprint={1907.06837},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.06837}, 
}

@misc{aher2023usinglargelanguagemodels,
      title={Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies}, 
      author={Gati Aher and Rosa I. Arriaga and Adam Tauman Kalai},
      year={2023},
      eprint={2208.10264},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.10264}, 
}

@misc{yue2024mathvcllmsimulatedmulticharactervirtual,
      title={MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education}, 
      author={Murong Yue and Wijdane Mifdal and Yixuan Zhang and Jennifer Suh and Ziyu Yao},
      year={2024},
      eprint={2404.06711},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06711}, 
}

@misc{ma2024weatherqamultimodallanguagemodels,
      title={WeatherQA: Can Multimodal Language Models Reason about Severe Weather?}, 
      author={Chengqian Ma and Zhanxiang Hua and Alexandra Anderson-Frey and Vikram Iyer and Xin Liu and Lianhui Qin},
      year={2024},
      eprint={2406.11217},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.11217}, 
}
